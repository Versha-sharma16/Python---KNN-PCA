{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Ans - K-Nearest Neighbors (KNN) is a simple, non-parametric, instance-based machine learning algorithm used for both classification and regression. It makes predictions based on the “similarity” between data points.\n",
        "\n",
        "###How KNN Works (General Idea)\n",
        "\n",
        "KNN does not build a model. Instead, it stores all training data and makes predictions by:\n",
        "\n",
        "Calculating the distance (usually Euclidean) between a new data point and all training points.\n",
        "\n",
        "Selecting the K closest (nearest) neighbors.\n",
        "\n",
        "Making a prediction based on those neighbors.\n",
        "\n",
        "###KNN for Classification\n",
        "\n",
        "In classification, KNN assigns a class label to a new point by majority vote among its K nearest neighbors.\n",
        "\n",
        "Steps\n",
        "\n",
        "Choose K (e.g., 3, 5, 7).\n",
        "\n",
        "Compute distance from the new sample to all training samples.\n",
        "\n",
        "Pick the K closest neighbors.\n",
        "\n",
        "Count how many neighbors belong to each class.\n",
        "\n",
        "Predict the most frequent class.\n",
        "\n",
        "Example\n",
        "\n",
        "If K = 5 and neighbors’ classes are:\n",
        "\n",
        "Class A: 3\n",
        "\n",
        "Class B: 2\n",
        "\n",
        "→ Prediction = Class A\n",
        "\n",
        "###KNN for Regression\n",
        "\n",
        "In regression, KNN predicts a numerical value rather than a class.\n",
        "- Steps\n",
        "\n",
        "Find the K nearest neighbors.\n",
        "\n",
        "Take the average (or weighted average) of their target values.\n",
        "\n",
        "Use this average as the prediction.\n",
        "\n",
        "Example\n",
        "\n",
        "Neighbors' target values: 10, 12, 13 (K=3)\n",
        "→ Predicted value = (10 + 12 + 13) / 3 = 11.67\n",
        "\n",
        "###Important Considerations\n",
        "\n",
        "Choosing K\n",
        "\n",
        "- Small K → sensitive to noise.\n",
        "- Large K → smoother but may blur class boundaries.\n",
        "\n",
        "###Distance Metrics\n",
        "\n",
        "Common options:\n",
        "\n",
        "- Euclidean distance\n",
        "- Manhattan distance\n",
        "- Minkowski distance\n",
        "- Cosine similarity (for high-dimensional data)\n",
        "\n",
        "###Feature Scaling\n",
        "\n",
        "KNN is distance-based → always use:\n",
        "\n",
        "- Normalization\n",
        "- Standardization\n",
        "\n",
        "###Advantages\n",
        "\n",
        "- Simple to understand\n",
        "- Works well with small to medium datasets\n",
        "- No training time\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- Slow prediction with large datasets\n",
        "- Sensitive to irrelevant features\n",
        "- Requires careful scaling\n",
        "\n",
        "---\n",
        "\n",
        "#Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Ans - The \"curse of dimensionality\" describes how the volume of the feature space grows exponentially with more dimensions, causing data to become sparse and increasing computational demands. This severely impacts the k-Nearest Neighbors (KNN) algorithm because the concept of \"nearest\" neighbors loses its local meaning in high-dimensional spaces, as data points become equidistant from one another. As a result, KNN becomes less effective, requiring significantly more data to achieve reliable results and increasing the risk of overfitting.\n",
        "\n",
        "##How the curse of dimensionality affects KNN\n",
        "\n",
        "- Loss of \"nearness\": In high-dimensional spaces, the distance between the closest and farthest points becomes very similar. This makes the notion of a \"nearest neighbor\" less meaningful, as points that are \"close\" by a distance metric might not be meaningfully similar in reality.\n",
        "\n",
        "- Increased data requirements: The data becomes sparse, meaning more data is needed to adequately sample the feature space and find reliable neighbors. The amount of data required grows exponentially with the number of dimensions, quickly becoming impractical.\n",
        "\n",
        "- Performance degradation: Due to the sparsity and loss of \"nearness,\" KNN's predictive accuracy decreases. The algorithm struggles to find relevant neighbors, and adding noisy or redundant features further degrades performance.\n",
        "\n",
        "- Computational complexity: Calculating distances and identifying neighbors becomes computationally more intensive as the number of dimensions increases. This makes KNN slower and less efficient for high-dimensional data.\n",
        "\n",
        "- Overfitting: With a high number of dimensions and insufficient data to represent the space, the algorithm can become overly sensitive to noise and random fluctuations in the training data, leading to overfitting.\n",
        "\n",
        "##How to mitigate the effect\n",
        "\n",
        "- Dimensionality reduction: Techniques like principal component analysis (PCA) can reduce the number of dimensions while retaining most of the data's variance.\n",
        "\n",
        "- Feature selection: This involves identifying and keeping only the most relevant features and removing the rest, which can improve performance and reduce computational cost.\n",
        "\n",
        "- Use other algorithms: For high-dimensional data, algorithms that are less sensitive to dimensionality, such as support vector machines or decision trees, may be more suitable alternatives.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Ans - Principal Component Analysis (PCA) is a dimensionality reduction technique that creates new, fewer features (called principal components) by taking linear combinations of original features, while feature selection keeps a subset of the original features. The key difference is that PCA creates new features and can make the original data less interpretable, whereas feature selection works with the original features and is generally more interpretable.\n",
        "\n",
        "##Principal Component Analysis (PCA)\n",
        "\n",
        "- What it is: A technique that transforms original features into a new set of linearly uncorrelated features called principal components.\n",
        "\n",
        "- How it works: It finds the directions (principal components) of maximum variance in the data. These components are ranked by the amount of variance they explain, allowing for dimensionality reduction by keeping only the most important components.\n",
        "\n",
        "- Outcome: A new dataset with fewer features, where each feature is a linear combination of the original ones. This can lead to a loss of original feature interpretability.\n",
        "\n",
        "- Example: Given features like \"height,\" \"weight,\" and \"age,\" PCA might create a new component like \"body mass index\" (PC1) and another component representing \"general age-related attributes\" (PC2).\n",
        "\n",
        "##Feature Selection\n",
        "\n",
        "- What it is: A process of selecting a subset of the most relevant features from the original dataset to use for a model.\n",
        "\n",
        "- How it works: It ranks the original input variables based on their importance for predicting a target variable and discards the rest.\n",
        "\n",
        "- Outcome: The original features are kept, so the model is built using only a subset of the original data. This maintains the original interpretability of the features.\n",
        "\n",
        "- Example: If you are predicting if a person will be a top athlete, feature selection might keep \"height\" and \"weight\" but discard \"age\" if it's found to be less relevant to the prediction goal.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "#Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Ans - In PCA, eigenvectors are the directions of maximum variance in the data, and eigenvalues are the magnitude of variance along those directions. They are important because they allow PCA to reduce the dimensionality of data by identifying and keeping the components with the most variance (eigenvectors with the largest eigenvalues) while discarding the components with the least variance.\n",
        "\n",
        "###Eigenvectors and Eigenvalues in PCA\n",
        "\n",
        "- Eigenvectors: These are the principal components, which represent the new axes of the data. The first eigenvector points in the direction of the largest variance, the second points in the direction of the second-largest variance, and so on.\n",
        "- Eigenvalues: These are scalar values that quantify the amount of variance captured by each corresponding eigenvector. A larger eigenvalue indicates that its eigenvector captures a greater amount of the data's variance.\n",
        "\n",
        "###Why they are important\n",
        "\n",
        "- Ranking of components: By ranking the eigenvalues from largest to smallest, you can rank the corresponding eigenvectors from most significant to least significant.\n",
        "\n",
        "- Dimensionality reduction: You can achieve dimensionality reduction by keeping only the eigenvectors with the largest eigenvalues. This is because these eigenvectors capture the most important patterns and variance in the data, while those with very small eigenvalues capture very little information.\n",
        "\n",
        "- Feature extraction: The eigenvectors (principal components) become the new features for a model, and the eigenvalues indicate the importance of each new feature.\n",
        "\n",
        "- Visualization and analysis: PCA can be used to reduce a high-dimensional dataset to two or three dimensions for visualization by keeping the top two or three principal components.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Ans - When PCA and KNN are applied in a single pipeline, PCA acts as a crucial preprocessing step that addresses several limitations of KNN, primarily related to the curse of dimensionality, leading to significant improvements in computational efficiency and, often, classification accuracy.\n",
        "\n",
        "###How PCA Complements KNN\n",
        "\n",
        "- Dimensionality Reduction: KNN's performance deteriorates in high-dimensional spaces (the \"curse of dimensionality\") because the distance metrics become less meaningful, and all data points appear to be roughly the same distance from each other. PCA mitigates this by transforming the data into a lower-dimensional space while retaining most of the important variance, allowing KNN to operate in a more meaningful feature space.\n",
        "\n",
        "- Improved Efficiency: Calculating distances between data points in high dimensions is computationally expensive and time-consuming. By reducing the number of features, PCA significantly speeds up the distance calculations required by KNN during both training (if applicable) and prediction phases.\n",
        "\n",
        "- Noise Reduction: PCA helps to denoise the data by focusing on the principal components that capture the most significant patterns and effectively removing \"background\" or irrelevant variations (noise) present in the original data. A cleaner feature set helps KNN to find more relevant neighbors and make better\n",
        "classifications.\n",
        "\n",
        "- Reduced Multicollinearity: The principal components generated by PCA are orthogonal (uncorrelated) by design. This removes multicollinearity among features, which can otherwise skew distance calculations in KNN.\n",
        "\n",
        "- Enhanced Accuracy: In many real-world applications, such as image or financial time series classification, the combination of PCA and KNN has been shown to achieve higher accuracy compared to using KNN alone, by enabling the algorithm to focus on the most discriminative features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "Ans -"
      ],
      "metadata": {
        "id": "PIyCY8GOVBtJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5-All3DU9u-",
        "outputId": "57359a69-7bc8-424d-8315-bc1868c4a886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.7222222222222222\n",
            "Accuracy WITH scaling:    0.9444444444444444\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# 1. KNN WITHOUT FEATURE SCALING\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "\n",
        "# 2. KNN WITH FEATURE SCALING\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy WITHOUT scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy WITH scaling:   \", acc_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n"
      ],
      "metadata": {
        "id": "wcJ2jfl6fTuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()  # keep all components\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, variance in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {variance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erl1-ed8fWIg",
        "outputId": "da603b55-0b30-4307-c1f8-5883ac71b437"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "31qSm_3YfaHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# 1. KNN ON ORIGINAL (SCALED)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "\n",
        "# 2. PCA (TOP 2 COMPONENTS)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on ORIGINAL dataset        :\", acc_original)\n",
        "print(\"Accuracy on PCA-transformed dataset :\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxl5DdAafnrR",
        "outputId": "acec6caa-6d11-4786-dc1c-2f46ce4a3a17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on ORIGINAL dataset        : 0.9444444444444444\n",
            "Accuracy on PCA-transformed dataset : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results\n"
      ],
      "metadata": {
        "id": "Wr1xO45FfuKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale data (important for distance-based models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# 1. KNN with EUCLIDEAN distance\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "\n",
        "# 2. KNN with MANHATTAN distance\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy with Euclidean distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y88q5afDf4Wp",
        "outputId": "ba5bf0d2-29c8-4299-ea27-0999200617f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444444444444444\n",
            "Accuracy with Manhattan distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "#classify patients with different types of cancer.\n",
        "#Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "#Explain how you would:\n",
        "#● Use PCA to reduce dimensionality\n",
        "#● Decide how many components to keep\n",
        "#● Use KNN for classification post-dimensionality reduction\n",
        "#● Evaluate the model\n",
        "#● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data."
      ],
      "metadata": {
        "id": "OBJod42SgAup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PCA + KNN Pipeline for Gene Expression Data\n",
        "# (Using Wine dataset as example placeholder)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine   # Replace with real gene expression data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1. Load dataset\n",
        "X, y = load_wine(return_X_y=True)   # Replace this with your gene expression matrix\n",
        "\n",
        "# 2. Build pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),    # retain 95% of variance\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# 3. Evaluate with Stratified CV\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", np.mean(scores))\n",
        "print(\"Std Dev:\", np.std(scores))\n",
        "\n",
        "# 4. Inspect PCA variance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca_full = PCA().fit(X_scaled)\n",
        "\n",
        "print(\"\\nExplained Variance Ratio by Component:\")\n",
        "print(pca_full.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH93j0Ycf9VD",
        "outputId": "0a84bd9a-fc73-48d7-c94d-42ec8c72d6f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.97222222 0.94444444 0.97222222 0.94285714 0.97142857]\n",
            "Mean Accuracy: 0.9606349206349206\n",
            "Std Dev: 0.013879588722983607\n",
            "\n",
            "Explained Variance Ratio by Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    }
  ]
}